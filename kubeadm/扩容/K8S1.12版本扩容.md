# 手动扩容K8S
#### 现在版本为1.12，记录扩容时遇到的问题，加深理解与印象
#### 现在已有3台master节点，希望变成5台master节点，节点包含etcd、kubelet、apiserver、controller-manager、schduler
#### 记录扩容时遇到的问题，加深理解与印象
#### 机器状况
```
//原集群                    组件
master01 10.10.33.37       etcd、kubelet、apiserver、controller-manager、schduler
master02 10.10.33.38       etcd、kubelet、apiserver、controller-manager、schduler
master03 10.10.33.39       etcd、kubelet、apiserver、controller-manager、schduler

//扩容集群
master04 10.10.33.40
master05 10.10.33.41
```

##### 1、原节点备份操作
```
//备份相关紧要数据
# mkdir /root/backup
# cd /root/backup
# cp -r /etc/etcd .
# cp -r /etc/kubernetes/ .
# export ETCDCTL_API=3 && /usr/local/bin/etcdctl --endpoints 127.0.0.1:2379 --cacert="/etc/etcd/ssl/ca.crt" --cert="/etc/etcd/ssl/client.crt" --key="/etc/etcd/ssl/client.key snapshot save etcd-backup.db
```

##### 2、扩容节点 ETCD 操作
```
//先关闭机器的防火墙、selinux，修改主机名
# systemctl stop firewalld && system disable firewalld && setenforce 0
# hostnamectl set-hostname master04

//通过二进制包安装 etcd
# tar xzf etcd-v3.2.20-linux-amd64.tar.gz
# cd etcd-v3.2.20-linux-amd64
# cp etcd* /usr/local/bin/

//etcd 安装完成后只能证明有这个二进文件了，但是启动文件还需要配置
# vi /etc/systemd/system/etcd.service
[Unit]
Description=etcd server
After=network.target

[Service]
WorkingDirectory=/var/lib/etcd
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
ExecStart=/usr/local/bin/etcd $ETCD_OPTS
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

//配置完启动文件后还有配置文件，这时 /etc 下面是没有 etcd 文件夹的，需要创建
//注：特别注意--initial-cluster-state设置为existing，这边为 existing 模式为注册到已经存在的集群中，如果不写的话则会创建一个新的集群
# mkdir /etc/etcd
# vi /etc/etcd/etcd.conf
ETCD_IMAGE_TAG=v3.2.20
ETCD_SSL_DIR=/etc/etcd/ssl/
ETCD_OPTS="--name master04 \
  --listen-client-urls https://10.10.33.40:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.10.33.40:2379 \
  --listen-peer-urls https://10.10.33.40:2380 \
  --initial-advertise-peer-urls https://10.10.33.40:2380 \
  --initial-cluster master01=https://10.10.33.37:2380,master02=https://10.10.33.38:2380,master03=https://10.10.33.39:2380,master04=https://10.10.33.40:2378 \
  --initial-cluster-token etcd-k8s-cluster \
  --initial-cluster-state existing \
  --client-cert-auth \
  --trusted-ca-file /etc/etcd/ssl/ca.crt \
  --cert-file /etc/etcd/ssl/server.crt \
  --key-file /etc/etcd/ssl/server.key \
  --peer-client-cert-auth \
  --peer-trusted-ca-file /etc/etcd/ssl/ca.crt \
  --peer-cert-file /etc/etcd/ssl/peer.crt \
  --peer-key-file /etc/etcd/ssl/peer.key \
  --auto-compaction-retention 1 \
  --data-dir=/var/lib/etcd
  --metrics basic"

//现在 etcd 的启动文件配了、配置文件配了、二进制文件配了，现在还有证书文件没配置，etcd权限没给，/var/lib/etcd存放数据的文件夹没建
//etcd证书文件有4对：
ca.crt         //ca就是证书的颁发机构
ca.key         //
client.crt     //用于通过服务器验证客户端。例如 etcdctLetcd proxy 等 
client.key     //
peer.crt       //由 etcd 集群成员使用，用于加密它们之间的通信
peer.key       //
server.crt     //由服务器使用，用于通过客户端验证服务器身份。例如 etcd 服务器
server.key     //

//这边可以通过 cfssl 创建证书，其中 client、peer、server 都是涉及 IP 地址，所以需要重新添加SubjectAltName,下面内容所以要重新生成证书
# vi client-csr.json
{
    "CN": "Etcd-server",
    "hosts": [
        "10.10.33.37",
        "10.10.33.38",
        "10.10.33.39",
        "10.10.33.40",
        "10.10.33.41",
        "127.0.0.1",
        "::1",
        "::"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "O": "autogenerated",
            "OU": "etcd cluster",
            "L": "the internet"
        }
    ]
}

# vi peer-csr.json
{
    "CN": "Etcd-server",
    "hosts": [
        "10.10.33.37",
        "10.10.33.38",
        "10.10.33.39",
        "10.10.33.40",
        "10.10.33.41",
        "127.0.0.1",
        "127.0.0.1",
        "::1",
        "::"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
         {
            "O": "autogenerated",
            "OU": "etcd cluster",
            "L": "the internet"
        }
    ]
}

# vi server-csr.json
{
    "CN": "Etcd-server",
    "hosts": [
        "10.10.33.37",
        "10.10.33.38",
        "10.10.33.39",
        "10.10.33.40",
        "10.10.33.41",
        "127.0.0.1",
        "::1",
        "::"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "O": "autogenerated",
            "OU": "etcd cluster",
            "L": "the internet"
        }
    ]
}

# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
# cfssl gencert \
            -ca=ca.pem \
            -ca-key=ca-key.pem \
            -config=ca-config.json \
            -profile=peer peer-csr.json | cfssljson -bare peer
# cfssl gencert \
            -ca=ca.pem \
            -ca-key=ca-key.pem \
            -config=ca-config.json \
            -profile=client client-csr.json | cfssljson -bare client
# cfssl gencert \
            -ca=ca.pem \
            -ca-key=ca-key.pem \
            -config=ca-config.json \
            -profile=server server-csr.json | cfssljson -bare server

//新环境新增证书
# mkdir /etc/etcd/ssl
# cp ca.pem /etc/etcd/ssl/ca.crt 
# cp ca-key.pem /etc/etcd/ssl/ca.key
# cp client.pem /etc/etcd/ssl/client.crt 
# cp client-key.pem /etc/etcd/ssl/client.key
# cp server.pem /etc/etcd/ssl/server.crt 
# cp server-key.pem /etc/etcd/ssl/server.key
# cp peer.pem /etc/etcd/ssl/peer.crt 
# cp peer-key.pem /etc/etcd/ssl/peer.key
//原有环境替换证书
# scp *.pem root@10.10.33.37:/etc/etcd/ssl/

//扩容节点创建文件夹，给权限
# mkdir /var/lib/etcd
# useradd etcd
# chown etcd:etcd /etc/etcd/ -R
# chown etcd:etcd /var/lib/etcd -R

//这时先不着急启动，需要在原有节点的 /etc/etcd/etcd.conf 里追加集群信息
//--initial-cluster 追加扩容节点 etcd 信息后重启 etcd，然后在通过命令加入把扩容节点 etcd 加入集群，扩容节点再启动集群
//master01 操作
# ETCDCTL_API=3 etcdctl --cacert="/etc/etcd/ssl/ca.crt" --cert="/etc/etcd/ssl/client.crt" --key="/etc/etcd/ssl/client.key" --endpoints="https://10.10.33.37:2379,https://10.10.33.38:2379,https://10.10.33.39:2379"  member add master04 --peer-urls=https://10.10.33.40:2380
```

##### 3、扩容节点 Kubernetes 操作
```
//yum 安装 docker、kubelet、kubectl
# yum install -y docker-ce kubelet kubectl

//配置 kubelet 启动文件
# vi /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \
        $KUBELET_ADDRESS \
        $KUBELET_POD_INFRA_CONTAINER \
        $KUBELET_ARGS \
        $KUBE_LOGTOSTDERR \
        $KUBE_ALLOW_PRIV \
        $KUBELET_NETWORK_ARGS \
        $KUBELET_DNS_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target

//复制原节点文件到扩容节点
# scp -r root@10.10.33.37:/etc/kubernetes /etc
# scp -r root@10.10.33.37:/etc/calico /etc
# scp -r root@10.10.33.37:/etc/cni /etc
# scp -r root@10.10.33.37:/opt/cni /opt
# scp -r root@10.10.33.37:/var/lib/calico /var/lib
# scp -r root@10.10.33.37:/var/lib/cni /var/lib
# scp -r root@10.10.33.37:/var/lib/kubelet /var/lib/kubelet

//删除无效文件
# rm -rf /var/lib/calico/nodename      //这边nodename里为主机名，calico获取到主机名后会键入到calico-node pod 中，
# touch /var/lib/calico/nodename       //但是这个不能人为修改，人为 vi 进去修改会有一个 \n 符，
                                       //这个时候 calico 会报错提示格式错误，因为它无法识别 \n

# rm -rf /var/lib/kubelet/pods/*       //因为文件是复制的所以这边 pod 下存在垃圾文件
# rm -rf /var/lib/kubelet/pki/*        //pki 下证书文件是自动生成的
# rm -rf /var/lib/device-plugins/*     //这边是无效的sock文件

//修改 apiserver、etcd 地址
# cd /etc/kubernetes/
# grep -rn "10.10.33.37"
manifests/kube-apiserver.json:49:	  "--advertise-address=10.10.33.37",
manifests/kube-apiserver.json:57:          "--etcd-servers=https://10.10.33.37:2379,https://10.10.33.38:2379,https://10.10.33.39:2379"
manifests/kube-controller-manager.json:30:          "--master=https://10.10.33.37:6443",
manifests/kube-scheduler.json:31:          "--master=https://10.10.33.37:6443"
kubeadminconfig:5:    server: https://10.10.33.37:6443
kubeadminconfig:10:    server: https://10.10.33.37:6443
kubeconfig:5:    server: 10.10.33.37
kubeconfig:10:    server: 10.10.33.37
admin.conf:6:    server: https://10.10.33.37:6443
addon/calico.yaml:16:  etcd_endpoints: "https://10.10.33.37:2379,https://10.10.33.38:2379,https://10.10.33.39:2379" //插件可以不用管

//这个时候扩容节点服务已经 yum 安装、kubelet 启动文件已配、/etc/kubernetes/ 下 apiserver、etcd 地址已配，就差证书可以让 kubelet 注册到 apiserver 中
```

##### 4、原节点 Kubernetes 证书操作
```
//修改 apiserver 证书文件，追加 SubjectAltName 配置
# echo IP.7 = 10.10.33.40 >> openssl.conf
# echo IP.8 = 10.10.33.41 >> openssl.conf

# cat openssl.conf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 127.0.0.1
IP.3 = 100.64.0.1
IP.4 = 10.10.33.37
IP.5 = 10.10.33.38
IP.6 = 10.10.33.39
IP.7 = 10.10.33.40
IP.8 = 10.10.33.41

# ls /opt/kubernetes/pki/
admin.csr  admin-key.pem  admin.pem  apiserver.csr  apiserver-key.pem  apiserver.pem  ca-key.pem  ca.pem  ca.srl  openssl.conf

# rm -rf apiserver.csr apiserver-key.pem

//生成新的 apiserver 证书文件
//先生成签名，再生成证书
openssl req -new -key /opt/kubernetes/pki/apiserver-key.pem -out /opt/kubernetes/pki/apiserver.csr -subj '/C=CN/ST=JiangSu/L=SuZhou/CN=master.example.com/emailAddress=admin@example.com/O=example/OU=dev' -config /opt/kubernetes/pki/openssl.conf
openssl x509 -req -in /opt/kubernetes/pki/apiserver.csr -CA /opt/kubernetes/pki/ca.pem -CAkey /opt/kubernetes/pki/ca-key.pem -CAcreateserial -out /opt/kubernetes/pki/apiserver.pem -days 36135 -extensions v3_req -extfile /opt/kubernetes/pki/openssl.conf

//替换 apiserver.pem
cp apiserver.pem /etc/kubernetes/pki/
```

##### 5、扩容节点 Kubernetes 证书操作
```
//复制所有原节点所有证书文件到 /etc/kubernetes/pki 下
scp -r 10.10.33.37:/etc/kubernetes/pki /etc/kubernetes/

//启动服务
# systemctl start docker && systemctl start kubelet && systemctl enable docker && systemctl kubelet

//查看集群
# kubectl get nodes
NAME                   STATUS                     ROLES    AGE     VERSION
master01.example.com   Ready,SchedulingDisabled   master   2d19h   v1.12.6
master02.example.com   Ready,SchedulingDisabled   master   2d19h   v1.12.6
master03.example.com   Ready,SchedulingDisabled   master   2d19h   v1.12.6
master04.example.com   Ready,SchedulingDisabled   master   2d19h   v1.12.6
master05.example.com   Ready,SchedulingDisabled   master   2d18h   v1.12.6
node01.example.com     Ready                      node     2d19h   v1.12.6
node02.example.com     Ready                      node     2d19h   v1.12.6
node03.example.com     Ready                      node     2d19h   v1.12.6

//查看集群状态OK后一定要通过 curl 访问一下 apiserver
kubectl describe secret -n kube-system default-token
curl -H "Authorization: Bearer $token -k https://10.10.33.40:6443/api/v1/

//替换calico secret 里的 etcd 证书
# kubectl -n kube-system get secrets calico-etcd-secrets -o yaml > /root/backup/calico-etcd-secrets.bak.yaml
# cp calico-etcd-secrets.bak.yaml calico-etcd-secrets-new.yaml
# cat /etc/etcd/ssl/ca.crt |base64 -w 0
# cat /etc/etcd/ssl/client.crt |base64 -w 0
# cat /etc/etcd/ssl/client.key |base64 -w 0
# vi calico-etcd-secrets-new.yaml
# kubectl -n kube-system apply -f calico-etcd-secrets-new.yaml

```

##### 6、node 节点 Kubernetes 操作
```
//node 节点的 kubelet 注册到 apiserver 我发现注册的为 127.0.0.1:6443
//这边并没有 vip 给 node 节点注册，所以每个 node 启动了一个 nginx-proxy 做代理，把 3 台 master 代理成本机 0.0.0.0:6443
//修改完 nginx 配置，重启代理 docker restart nginx-proxy
# vi /opt/nginx/nginx.conf
error_log stderr notice;

worker_processes auto;
events {
  multi_accept on;
  use epoll;
  worker_connections 1024;
}

stream {
    upstream kube_apiserver {
        least_conn;
                        server 10.10.33.37:6443;
                server 10.10.33.38:6443;
                server 10.10.33.39:6443;
                server 10.10.33.40:6443;
                server 10.10.33.41:6443;
            }

    server {
        listen        0.0.0.0:6443;
        proxy_pass    kube_apiserver;
        proxy_timeout 10m;
        proxy_connect_timeout 1s;
    }
}
```

#### FAQ
```
这些步骤我操作了几天，遇到的坑也说明一下。

etcd 为什么要 5 节点，6 节点可不可以呢？
为什么 etcd 集群大家都说单数？
etcd 集群的机制是状态 ok 的节点必须超过集群的一半。
如果集群是 3 个，那么集群坏掉 1 个，集群可以照常访问，如果坏掉 2 个，集群就无法访问。
如果集群是 4 个，那么集群坏掉 1 个，集群可以照常访问，如果坏掉 2 个，集群就无法访问，坏掉 2 个，ok 状态只是等于集群的一半并没有大于。
所以 5 个跟 6 个集群可以损坏的数量一致，但是 6 个却需要多一个节点进行数据的均衡

关于证书，为什么 kubernetes 要重新生成 apiserver 证书呢 ？
k8s证书这边我只替换了 apiserver 的证书文件，其他的都没有替换，那是因为只有 apiserver 签名与证书用到了 openssl.conf 文件，
为什么只有我这边也只是在 openssl 里追加了两个 ip，其它的都没有变过？
客户端访问页面 https 时会出现不是信任的连接，需要你手动的信任，这边原理也是如此，
如果这边不追加 ip 地址，却把这个证书文件给了 apiserver 后，那么访问这个 ip 的 apiserver 后，就会显示这个 ip 地址不被证书支持。

K8S扩容初期我创建全新的ca、证书文件、私钥给扩容节点使用，但是却没有给原节点使用，这个时候发现集群也能注册上，但是通过token请求 apiserver 时请求不通，导致 daemonset 服务里访问 apiserver 的服务有部分报错有部分可用，报错为认证失败。
扩容节点 kubelet 注册到 本机的 apiserver 成功了，apiserver 访问 etcd 服务也成功了，
在扩容节点 kubectl get nodes 时其实时通过 apiserver 把 etcd里面的数据读出来了而已，这边形成了两个集群共用一套数据库数据一致，但集群信息证书不一致的情况。
当然也可以把所有全新的 ca 复制到原 master 上，这个时候需要改动就较大了，secret 下面的 token 是 serviceaccount 生成的，
这边用的是 ca 文件生成的 token，有顺序的替换所有证书，删除所有token，重建所有 apiserver、controller-manager、scheduler等，
node节点kubelet也要替换 ca，重新注册，其过程并不清楚哪些要换哪些不要换，过程繁琐且不清晰，可能会导致业务中断。
```